{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Artificial Neural Networks\n",
    "In this notebook we will be working in tensorflow with artificial neural networks to try and see if we can achieve a better accuracy than the models we had in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# filter warnings  (some packages produce lots of warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_csv('data/clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner_is_blue</th>\n",
       "      <th>is_title_bout</th>\n",
       "      <th>no_of_rounds</th>\n",
       "      <th>lose_streak_dif</th>\n",
       "      <th>win_streak_dif</th>\n",
       "      <th>win_dif</th>\n",
       "      <th>loss_dif</th>\n",
       "      <th>total_title_bout_dif</th>\n",
       "      <th>ko_dif</th>\n",
       "      <th>sub_dif</th>\n",
       "      <th>height_dif</th>\n",
       "      <th>reach_dif</th>\n",
       "      <th>age_dif</th>\n",
       "      <th>sig_str_dif</th>\n",
       "      <th>avg_sub_att_dif</th>\n",
       "      <th>avg_td_dif</th>\n",
       "      <th>empty_arena</th>\n",
       "      <th>total_fight_time_secs</th>\n",
       "      <th>finish_DQ</th>\n",
       "      <th>finish_KO/TKO</th>\n",
       "      <th>finish_M-DEC</th>\n",
       "      <th>finish_Overturned</th>\n",
       "      <th>finish_S-DEC</th>\n",
       "      <th>finish_SUB</th>\n",
       "      <th>finish_U-DEC</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>Chile</th>\n",
       "      <th>China</th>\n",
       "      <th>Croatia</th>\n",
       "      <th>Czech Republic</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Ireland</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Mexico</th>\n",
       "      <th>Netherlands</th>\n",
       "      <th>New Zealand</th>\n",
       "      <th>Philippines</th>\n",
       "      <th>Poland</th>\n",
       "      <th>Russia</th>\n",
       "      <th>Singapore</th>\n",
       "      <th>South Korea</th>\n",
       "      <th>Sweden</th>\n",
       "      <th>USA</th>\n",
       "      <th>United Arab Emirates</th>\n",
       "      <th>United Kingdom</th>\n",
       "      <th>Uruguay</th>\n",
       "      <th>Catch Weight</th>\n",
       "      <th>Featherweight</th>\n",
       "      <th>Flyweight</th>\n",
       "      <th>Heavyweight</th>\n",
       "      <th>Light Heavyweight</th>\n",
       "      <th>Lightweight</th>\n",
       "      <th>Middleweight</th>\n",
       "      <th>Welterweight</th>\n",
       "      <th>Women's Bantamweight</th>\n",
       "      <th>Women's Featherweight</th>\n",
       "      <th>Women's Flyweight</th>\n",
       "      <th>Women's Strawweight</th>\n",
       "      <th>B_Southpaw</th>\n",
       "      <th>B_Switch</th>\n",
       "      <th>R_Southpaw</th>\n",
       "      <th>R_Switch</th>\n",
       "      <th>better_rank_Red</th>\n",
       "      <th>weight_dif</th>\n",
       "      <th>ev_dif</th>\n",
       "      <th>draw_dif</th>\n",
       "      <th>avg_SIG_STR_pct_dif</th>\n",
       "      <th>avg_TD_pct_dif</th>\n",
       "      <th>win_by_Decision_Majority_dif</th>\n",
       "      <th>win_by_Decision_Split_dif</th>\n",
       "      <th>win_by_TKO_Doctor_Stoppage_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2788</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-7.62</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>0.338000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>-10.16</td>\n",
       "      <td>-4</td>\n",
       "      <td>-8.846154</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.069930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.054038</td>\n",
       "      <td>-0.288558</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-12.70</td>\n",
       "      <td>-7.62</td>\n",
       "      <td>-1</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>272.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.436667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.16</td>\n",
       "      <td>-2</td>\n",
       "      <td>-18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-128.717949</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.145000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.08</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>3</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>859.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-353.723404</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.095000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      winner_is_blue  is_title_bout  no_of_rounds  lose_streak_dif  \\\n",
       "2788               0              0             3               -1   \n",
       "3656               1              0             3               -1   \n",
       "2104               0              0             3                0   \n",
       "3415               1              0             3                0   \n",
       "1851               1              0             3                0   \n",
       "\n",
       "      win_streak_dif  win_dif  loss_dif  total_title_bout_dif  ko_dif  \\\n",
       "2788              -1       -2         2                    -1      -1   \n",
       "3656              -1        3        -2                     1      -1   \n",
       "2104               0        0         0                     0      -1   \n",
       "3415               0       -1         0                     0      -1   \n",
       "1851               0       -1         0                     0      -1   \n",
       "\n",
       "      sub_dif  height_dif  reach_dif  age_dif  sig_str_dif  avg_sub_att_dif  \\\n",
       "2788        0        0.00      -7.62        1    -5.000000         0.800000   \n",
       "3656        5       -2.54     -10.16       -4    -8.846154         0.500000   \n",
       "2104        0      -12.70      -7.62       -1   -10.000000        -0.333333   \n",
       "3415        0        0.00      10.16       -2   -18.000000         0.000000   \n",
       "1851        0       -5.08      -2.54        3    51.500000         0.000000   \n",
       "\n",
       "      avg_td_dif  empty_arena  total_fight_time_secs  finish_DQ  \\\n",
       "2788   -0.200000            0                  269.0          0   \n",
       "3656   -1.076923            0                   47.0          0   \n",
       "2104    1.000000            0                  285.0          0   \n",
       "3415   -0.500000            0                  900.0          0   \n",
       "1851    2.000000            0                  859.0          0   \n",
       "\n",
       "      finish_KO/TKO  finish_M-DEC  finish_Overturned  finish_S-DEC  \\\n",
       "2788              0             0                  0             0   \n",
       "3656              1             0                  0             0   \n",
       "2104              1             0                  0             0   \n",
       "3415              0             0                  0             0   \n",
       "1851              1             0                  0             0   \n",
       "\n",
       "      finish_SUB  finish_U-DEC  year  month  day  Australia  Brazil  Canada  \\\n",
       "2788           1             0  2014      9   13          0       1       0   \n",
       "3656           0             0  2012      7    7          0       0       0   \n",
       "2104           0             0  2016      2   27          0       0       0   \n",
       "3415           0             1  2013      4   13          0       0       0   \n",
       "1851           0             0  2016      9   17          0       0       0   \n",
       "\n",
       "      Chile  China  Croatia  Czech Republic  Denmark  Germany  Ireland  Japan  \\\n",
       "2788      0      0        0               0        0        0        0      0   \n",
       "3656      0      0        0               0        0        0        0      0   \n",
       "2104      0      0        0               0        0        0        0      0   \n",
       "3415      0      0        0               0        0        0        0      0   \n",
       "1851      0      0        0               0        0        0        0      0   \n",
       "\n",
       "      Mexico  Netherlands  New Zealand  Philippines  Poland  Russia  \\\n",
       "2788       0            0            0            0       0       0   \n",
       "3656       0            0            0            0       0       0   \n",
       "2104       0            0            0            0       0       0   \n",
       "3415       0            0            0            0       0       0   \n",
       "1851       0            0            0            0       0       0   \n",
       "\n",
       "      Singapore  South Korea  Sweden  USA  United Arab Emirates  \\\n",
       "2788          0            0       0    0                     0   \n",
       "3656          0            0       0    1                     0   \n",
       "2104          0            0       0    0                     0   \n",
       "3415          0            0       0    1                     0   \n",
       "1851          0            0       0    1                     0   \n",
       "\n",
       "      United Kingdom  Uruguay  Catch Weight  Featherweight  Flyweight  \\\n",
       "2788               0        0             0              1          0   \n",
       "3656               0        0             0              0          0   \n",
       "2104               1        0             0              0          0   \n",
       "3415               0        0             0              1          0   \n",
       "1851               0        0             0              0          0   \n",
       "\n",
       "      Heavyweight  Light Heavyweight  Lightweight  Middleweight  Welterweight  \\\n",
       "2788            0                  0            0             0             0   \n",
       "3656            0                  0            0             0             1   \n",
       "2104            0                  0            0             1             0   \n",
       "3415            0                  0            0             0             0   \n",
       "1851            0                  0            0             0             1   \n",
       "\n",
       "      Women's Bantamweight  Women's Featherweight  Women's Flyweight  \\\n",
       "2788                     0                      0                  0   \n",
       "3656                     0                      0                  0   \n",
       "2104                     0                      0                  0   \n",
       "3415                     0                      0                  0   \n",
       "1851                     0                      0                  0   \n",
       "\n",
       "      Women's Strawweight  B_Southpaw  B_Switch  R_Southpaw  R_Switch  \\\n",
       "2788                    0           0         0           0         0   \n",
       "3656                    0           1         0           1         0   \n",
       "2104                    0           0         0           1         0   \n",
       "3415                    0           0         0           0         0   \n",
       "1851                    0           0         0           0         0   \n",
       "\n",
       "      better_rank_Red  weight_dif      ev_dif  draw_dif  avg_SIG_STR_pct_dif  \\\n",
       "2788                0           0   43.076923         0             0.338000   \n",
       "3656                0           0   63.069930         0            -0.054038   \n",
       "2104                0           0  272.222222         0            -0.436667   \n",
       "3415                0           0 -128.717949         0            -0.145000   \n",
       "1851                0           0 -353.723404         0            -0.095000   \n",
       "\n",
       "      avg_TD_pct_dif  win_by_Decision_Majority_dif  win_by_Decision_Split_dif  \\\n",
       "2788       -0.100000                             0                         -1   \n",
       "3656       -0.288558                             0                         -1   \n",
       "2104        0.083333                             0                          1   \n",
       "3415       -0.500000                             0                          0   \n",
       "1851        0.250000                             0                          0   \n",
       "\n",
       "      win_by_TKO_Doctor_Stoppage_dif  \n",
       "2788                               0  \n",
       "3656                               0  \n",
       "2104                               0  \n",
       "3415                               0  \n",
       "1851                               0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set option to see all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# check\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use imbalance learn's random oversampling method to correct the imbalance in our dependent variable before getting into modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our X and y\n",
    "X = df.drop(['winner_is_blue'], axis = 1)\n",
    "y = df['winner_is_blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random over sampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2541\n",
       "0    2541\n",
       "Name: winner_is_blue, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "# check new balance\n",
    "y_over.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataset from oversampling\n",
    "df = pd.concat([X_over, y_over], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping the year column:\n",
    "After trying different methods to split our data, the train_test_split method in sklearn gave me the best results. Ergo, I will drop the year column to basically avoid predicting 'past' results from 'future' matches. The year should not have any major predictive role anyways. Another method of splitting that I tried was to make 2019 and 2020 my test, 2018 my validation and the rest my train data. However, the results are more conistent here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year\n",
    "df.drop(['year'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform train_test_split and scale:\n",
    "As mentioned, we will use sklearns method of randomly creating train test split tests. We will then scale the data. Neural networks are heavily reliant on scaling since the different weights from the inputs will be very different if not scaled and that can skew a models' predictions. Here the standard scaler proved to be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our X and y\n",
    "X = df.drop(['winner_is_blue', 'total_fight_time_secs', 'finish_DQ', 'finish_KO/TKO', 'finish_M-DEC', 'finish_Overturned',\n",
    "             'finish_S-DEC', 'finish_SUB', 'finish_U-DEC'], axis = 1)\n",
    "y = df['winner_is_blue']\n",
    "\n",
    "# create test and remainder sets\n",
    "X_remainder, X_test, y_remainder, y_test = \\\n",
    "    train_test_split(X, y, test_size = 0.15,\n",
    "                     random_state=1)\n",
    "\n",
    "# Splitting the remainder in two chunks\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_remainder, y_remainder, test_size = 0.15,\n",
    "                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22839427, -0.31745068, -0.17410208, ...,  0.06959347,\n",
       "         0.13795323,  0.06657221],\n",
       "       [-0.22839427, -0.31745068, -0.17410208, ...,  0.06959347,\n",
       "         0.13795323,  0.06657221],\n",
       "       [-0.22839427, -0.31745068,  0.87022593, ...,  0.06959347,\n",
       "        -2.30855061,  0.06657221],\n",
       "       ...,\n",
       "       [-0.22839427, -0.31745068, -2.26275811, ...,  0.06959347,\n",
       "         0.13795323,  0.06657221],\n",
       "       [-0.22839427, -0.31745068, -1.2184301 , ...,  0.06959347,\n",
       "         0.13795323,  0.06657221],\n",
       "       [-0.22839427, -0.31745068,  0.87022593, ...,  0.06959347,\n",
       "         0.13795323,  0.06657221]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the models:\n",
    "Lets start by creating many layers with 'relu' as their activation function and 'sigmoid' as the output's activation function since we have a binary. Other activation functions were also tested, however relu and sigmoid seemed to be our best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6723 - accuracy: 0.5876 - val_loss: 0.6613 - val_accuracy: 0.5972\n",
      "Epoch 2/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6262 - accuracy: 0.6540 - val_loss: 0.6454 - val_accuracy: 0.6265\n",
      "Epoch 3/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5911 - accuracy: 0.6835 - val_loss: 0.6496 - val_accuracy: 0.5818\n",
      "Epoch 4/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.7093 - val_loss: 0.6509 - val_accuracy: 0.6327\n",
      "Epoch 5/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7461 - val_loss: 0.6892 - val_accuracy: 0.6373\n",
      "Epoch 6/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.7804 - val_loss: 0.7742 - val_accuracy: 0.6404\n",
      "Epoch 7/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.4168 - accuracy: 0.8112 - val_loss: 0.8294 - val_accuracy: 0.6574\n",
      "Epoch 8/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.8409 - val_loss: 0.7788 - val_accuracy: 0.6127\n",
      "Epoch 9/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2971 - accuracy: 0.8752 - val_loss: 1.0150 - val_accuracy: 0.6759\n",
      "Epoch 10/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9087 - val_loss: 1.0731 - val_accuracy: 0.6559\n",
      "Epoch 11/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9169 - val_loss: 1.1382 - val_accuracy: 0.6574\n",
      "Epoch 12/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1734 - accuracy: 0.9371 - val_loss: 1.1649 - val_accuracy: 0.6713\n",
      "Epoch 13/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1499 - accuracy: 0.9398 - val_loss: 1.3116 - val_accuracy: 0.6836\n",
      "Epoch 14/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9567 - val_loss: 1.7792 - val_accuracy: 0.6867\n",
      "Epoch 15/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.1019 - accuracy: 0.9600 - val_loss: 1.6618 - val_accuracy: 0.6620\n",
      "Epoch 16/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0951 - accuracy: 0.9689 - val_loss: 1.6854 - val_accuracy: 0.6914\n",
      "Epoch 17/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9738 - val_loss: 2.0017 - val_accuracy: 0.6559\n",
      "Epoch 18/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0685 - accuracy: 0.9758 - val_loss: 1.9613 - val_accuracy: 0.6744\n",
      "Epoch 19/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9790 - val_loss: 1.9351 - val_accuracy: 0.6744\n",
      "Epoch 20/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9867 - val_loss: 2.4860 - val_accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9861 - val_loss: 2.1338 - val_accuracy: 0.6651\n",
      "Epoch 22/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9834 - val_loss: 2.2550 - val_accuracy: 0.6435\n",
      "Epoch 23/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 0.9820 - val_loss: 2.5560 - val_accuracy: 0.6481\n",
      "Epoch 24/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9788 - val_loss: 1.6100 - val_accuracy: 0.6744\n",
      "Epoch 25/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9760 - val_loss: 1.6689 - val_accuracy: 0.6651\n",
      "Epoch 26/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.9858 - val_loss: 1.9117 - val_accuracy: 0.6528\n",
      "Epoch 27/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9788 - val_loss: 1.9541 - val_accuracy: 0.6528\n",
      "Epoch 28/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0676 - accuracy: 0.9798 - val_loss: 2.0073 - val_accuracy: 0.6451\n",
      "Epoch 29/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9856 - val_loss: 2.5396 - val_accuracy: 0.6528\n",
      "Epoch 30/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0343 - accuracy: 0.9894 - val_loss: 2.3346 - val_accuracy: 0.6698\n",
      "Epoch 31/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0337 - accuracy: 0.9916 - val_loss: 1.9826 - val_accuracy: 0.6620\n",
      "Epoch 32/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.9948 - val_loss: 2.6028 - val_accuracy: 0.6497\n",
      "Epoch 33/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0525 - accuracy: 0.9820 - val_loss: 1.8298 - val_accuracy: 0.6713\n",
      "Epoch 34/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9907 - val_loss: 2.4861 - val_accuracy: 0.6759\n",
      "Epoch 35/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9905 - val_loss: 2.0387 - val_accuracy: 0.6759\n",
      "Epoch 36/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.9913 - val_loss: 2.3715 - val_accuracy: 0.6728\n",
      "Epoch 37/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0377 - accuracy: 0.9899 - val_loss: 2.0576 - val_accuracy: 0.6898\n",
      "Epoch 38/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9880 - val_loss: 2.1346 - val_accuracy: 0.6636\n",
      "Epoch 39/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9858 - val_loss: 1.6685 - val_accuracy: 0.6975\n",
      "Epoch 40/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 2.0980 - val_accuracy: 0.6713\n",
      "Epoch 41/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9913 - val_loss: 2.6468 - val_accuracy: 0.6759\n",
      "Epoch 42/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0352 - accuracy: 0.9896 - val_loss: 2.7372 - val_accuracy: 0.6528\n",
      "Epoch 43/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9905 - val_loss: 2.5545 - val_accuracy: 0.6852\n",
      "Epoch 44/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 0.9929 - val_loss: 2.2091 - val_accuracy: 0.6620\n",
      "Epoch 45/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9910 - val_loss: 2.1195 - val_accuracy: 0.6528\n",
      "Epoch 46/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.9899 - val_loss: 2.3882 - val_accuracy: 0.6744\n",
      "Epoch 47/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.9858 - val_loss: 2.9732 - val_accuracy: 0.6852\n",
      "Epoch 48/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 2.2692 - val_accuracy: 0.6667\n",
      "Epoch 49/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0377 - accuracy: 0.9905 - val_loss: 2.4584 - val_accuracy: 0.6744\n",
      "Epoch 50/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0435 - accuracy: 0.9867 - val_loss: 2.3795 - val_accuracy: 0.6497\n",
      "Epoch 51/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9935 - val_loss: 2.3369 - val_accuracy: 0.6728\n",
      "Epoch 52/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 0.9883 - val_loss: 2.1030 - val_accuracy: 0.6883\n",
      "Epoch 53/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0417 - accuracy: 0.9850 - val_loss: 2.6558 - val_accuracy: 0.6404\n",
      "Epoch 54/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 2.4321 - val_accuracy: 0.6728\n",
      "Epoch 55/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 2.8185 - val_accuracy: 0.6667\n",
      "Epoch 56/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.9926 - val_loss: 2.2577 - val_accuracy: 0.6667\n",
      "Epoch 57/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9910 - val_loss: 2.4602 - val_accuracy: 0.6682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9905 - val_loss: 2.1981 - val_accuracy: 0.6682\n",
      "Epoch 59/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 0.9907 - val_loss: 2.7058 - val_accuracy: 0.6512\n",
      "Epoch 60/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9891 - val_loss: 2.4019 - val_accuracy: 0.6759\n",
      "Epoch 61/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.9913 - val_loss: 2.7428 - val_accuracy: 0.6898\n",
      "Epoch 62/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.9951 - val_loss: 3.2906 - val_accuracy: 0.6836\n",
      "Epoch 63/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9913 - val_loss: 1.8488 - val_accuracy: 0.6698\n",
      "Epoch 64/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.9910 - val_loss: 2.4117 - val_accuracy: 0.6713\n",
      "Epoch 65/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.9959 - val_loss: 2.6522 - val_accuracy: 0.6682\n",
      "Epoch 66/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9948 - val_loss: 2.6149 - val_accuracy: 0.6574\n",
      "Epoch 67/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 2.9395 - val_accuracy: 0.6590\n",
      "Epoch 68/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9948 - val_loss: 2.8791 - val_accuracy: 0.6620\n",
      "Epoch 69/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9959 - val_loss: 3.0779 - val_accuracy: 0.6821\n",
      "Epoch 70/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 4.0499 - val_accuracy: 0.6821\n",
      "Epoch 71/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.9905 - val_loss: 2.4323 - val_accuracy: 0.6543\n",
      "Epoch 72/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0463 - accuracy: 0.9883 - val_loss: 2.1727 - val_accuracy: 0.6898\n",
      "Epoch 73/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9935 - val_loss: 1.7454 - val_accuracy: 0.6590\n",
      "Epoch 74/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.9921 - val_loss: 2.2289 - val_accuracy: 0.6528\n",
      "Epoch 75/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0359 - accuracy: 0.9867 - val_loss: 1.9800 - val_accuracy: 0.6775\n",
      "Epoch 76/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.9905 - val_loss: 2.9956 - val_accuracy: 0.6466\n",
      "Epoch 77/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 0.9839 - val_loss: 2.9618 - val_accuracy: 0.6775\n",
      "Epoch 78/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0386 - accuracy: 0.9888 - val_loss: 1.5691 - val_accuracy: 0.6636\n",
      "Epoch 79/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9839 - val_loss: 2.7082 - val_accuracy: 0.6775\n",
      "Epoch 80/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 2.6242 - val_accuracy: 0.6775\n",
      "Epoch 81/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.9929 - val_loss: 3.0980 - val_accuracy: 0.6512\n",
      "Epoch 82/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.9940 - val_loss: 2.4230 - val_accuracy: 0.6404\n",
      "Epoch 83/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.9929 - val_loss: 2.6607 - val_accuracy: 0.6512\n",
      "Epoch 84/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.9940 - val_loss: 2.8923 - val_accuracy: 0.6605\n",
      "Epoch 85/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9970 - val_loss: 2.5931 - val_accuracy: 0.6867\n",
      "Epoch 86/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.9929 - val_loss: 3.3690 - val_accuracy: 0.6790\n",
      "Epoch 87/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 2.6409 - val_accuracy: 0.6713\n",
      "Epoch 88/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9946 - val_loss: 2.8850 - val_accuracy: 0.6620\n",
      "Epoch 89/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9946 - val_loss: 3.5712 - val_accuracy: 0.6728\n",
      "Epoch 90/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9959 - val_loss: 3.1814 - val_accuracy: 0.6651\n",
      "Epoch 91/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 4.5058 - val_accuracy: 0.6574\n",
      "Epoch 92/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9970 - val_loss: 2.4204 - val_accuracy: 0.6620\n",
      "Epoch 93/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9921 - val_loss: 2.8116 - val_accuracy: 0.6667\n",
      "Epoch 94/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0335 - accuracy: 0.9913 - val_loss: 2.8208 - val_accuracy: 0.6775\n",
      "Epoch 95/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9967 - val_loss: 3.3197 - val_accuracy: 0.6559\n",
      "Epoch 96/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 0.9902 - val_loss: 2.3496 - val_accuracy: 0.6651\n",
      "Epoch 97/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9896 - val_loss: 1.3178 - val_accuracy: 0.6790\n",
      "Epoch 98/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.9886 - val_loss: 2.7212 - val_accuracy: 0.6852\n",
      "Epoch 99/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 2.6595 - val_accuracy: 0.6728\n",
      "Epoch 100/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9951 - val_loss: 3.0605 - val_accuracy: 0.6420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b4c41af8e0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize\n",
    "ann = tf.keras.models.Sequential()\n",
    "# add input layer and hidden layer\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "# add output layer\n",
    "ann.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "ann.compile(\n",
    "    optimizer= 'adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Train the model\n",
    "ann.fit(X_train, \n",
    "        y_train, \n",
    "        batch_size = 32,\n",
    "        epochs=100, \n",
    "        verbose=1, \n",
    "        # Pass in a validation data set to test each epoch.\n",
    "        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now add kernel regularizers to see if they can improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 2.7240 - accuracy: 0.4996 - val_loss: 1.1793 - val_accuracy: 0.5031\n",
      "Epoch 2/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.8602 - accuracy: 0.4911 - val_loss: 0.7188 - val_accuracy: 0.5031\n",
      "Epoch 3/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7005 - accuracy: 0.4917 - val_loss: 0.6938 - val_accuracy: 0.5031\n",
      "Epoch 4/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 5/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4884 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 6/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 7/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 8/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4950 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 9/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4838 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 10/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4947 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 11/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4922 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 12/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4846 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 13/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 14/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 15/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4974 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 16/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4903 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 17/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 18/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 19/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4911 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 20/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 21/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 22/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4890 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 23/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4816 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 24/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 25/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4841 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 26/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 27/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4922 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 28/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4974 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 29/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4879 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 30/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4974 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 31/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4849 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 32/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4961 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 33/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4936 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 34/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 35/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 36/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 37/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 38/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4917 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 39/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4827 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 40/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 41/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4841 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 42/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4944 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 43/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4917 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 44/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4950 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 45/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4879 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 46/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 47/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4901 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 48/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 49/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 50/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 51/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4868 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 52/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4920 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 53/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 54/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 55/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4901 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 56/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4939 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 57/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4890 - val_loss: 0.6931 - val_accuracy: 0.5031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 59/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4816 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 60/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 61/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 62/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4971 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 63/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4892 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 64/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4966 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 65/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4759 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 66/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4898 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 67/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4944 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 68/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 69/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 70/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 71/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 72/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 73/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4890 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 74/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4944 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 75/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4901 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 76/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 77/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 78/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 79/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 80/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4884 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 81/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4811 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 82/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4928 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 83/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 84/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 85/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4901 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 86/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4873 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 87/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4933 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 88/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 89/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4843 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 90/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 91/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4936 - val_loss: 0.6931 - val_accuracy: 0.4969\n",
      "Epoch 92/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4922 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 93/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 94/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 95/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4969 - val_loss: 0.6932 - val_accuracy: 0.4969\n",
      "Epoch 96/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4961 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 97/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 98/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4903 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 99/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5031\n",
      "Epoch 100/100\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6931 - val_accuracy: 0.5031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b4c872ca00>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "# Create a regularizer with a factor of 0.005 and apply it to all hidden layers\n",
    "regularizer = tf.keras.regularizers.l2(0.005)\n",
    "\n",
    "# add input layer and hidden layer\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu', kernel_regularizer=regularizer))\n",
    "# add output layer\n",
    "ann.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "ann.compile(\n",
    "    optimizer= 'adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Train the model\n",
    "ann.fit(X_train, \n",
    "        y_train, \n",
    "        batch_size = 32,\n",
    "        epochs=100, \n",
    "        verbose=1, \n",
    "        # Pass in a validation data set to test each epoch.\n",
    "        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model got much worse with the regularizers. Lets now to add more nodes to see how that changes things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "115/115 [==============================] - 1s 5ms/step - loss: 0.6824 - accuracy: 0.5484 - val_loss: 0.6785 - val_accuracy: 0.6420\n",
      "Epoch 2/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6389 - accuracy: 0.6440 - val_loss: 0.6251 - val_accuracy: 0.6620\n",
      "Epoch 3/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.6068 - accuracy: 0.6728 - val_loss: 0.6179 - val_accuracy: 0.6620\n",
      "Epoch 4/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.5620 - accuracy: 0.7099 - val_loss: 0.6629 - val_accuracy: 0.6127\n",
      "Epoch 5/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.5261 - accuracy: 0.7344 - val_loss: 0.6779 - val_accuracy: 0.6559\n",
      "Epoch 6/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.4713 - accuracy: 0.7736 - val_loss: 0.6827 - val_accuracy: 0.6713\n",
      "Epoch 7/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8041 - val_loss: 0.8499 - val_accuracy: 0.6451\n",
      "Epoch 8/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.3650 - accuracy: 0.8431 - val_loss: 0.7873 - val_accuracy: 0.6235\n",
      "Epoch 9/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.3095 - accuracy: 0.8630 - val_loss: 0.7595 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.8861 - val_loss: 1.2480 - val_accuracy: 0.6682\n",
      "Epoch 11/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9164 - val_loss: 1.4222 - val_accuracy: 0.6759\n",
      "Epoch 12/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1919 - accuracy: 0.9289 - val_loss: 1.4473 - val_accuracy: 0.6682\n",
      "Epoch 13/100\n",
      "115/115 [==============================] - 1s 4ms/step - loss: 0.1509 - accuracy: 0.9450 - val_loss: 1.7019 - val_accuracy: 0.6898\n",
      "Epoch 14/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9570 - val_loss: 1.1833 - val_accuracy: 0.6775\n",
      "Epoch 15/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.9616 - val_loss: 1.4606 - val_accuracy: 0.6790\n",
      "Epoch 16/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9736 - val_loss: 1.5957 - val_accuracy: 0.6713\n",
      "Epoch 17/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.9741 - val_loss: 1.7991 - val_accuracy: 0.6605\n",
      "Epoch 18/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9717 - val_loss: 1.9617 - val_accuracy: 0.6790\n",
      "Epoch 19/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 0.9706 - val_loss: 1.9610 - val_accuracy: 0.7145\n",
      "Epoch 20/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9847 - val_loss: 1.7746 - val_accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.9877 - val_loss: 2.0840 - val_accuracy: 0.7068\n",
      "Epoch 22/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9777 - val_loss: 1.7786 - val_accuracy: 0.7022\n",
      "Epoch 23/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9834 - val_loss: 2.3842 - val_accuracy: 0.6852\n",
      "Epoch 24/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9801 - val_loss: 2.8825 - val_accuracy: 0.6497\n",
      "Epoch 25/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0473 - accuracy: 0.9856 - val_loss: 1.6640 - val_accuracy: 0.6620\n",
      "Epoch 26/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.9935 - val_loss: 2.1829 - val_accuracy: 0.6883\n",
      "Epoch 27/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9861 - val_loss: 2.4401 - val_accuracy: 0.6728\n",
      "Epoch 28/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9921 - val_loss: 1.8546 - val_accuracy: 0.7145\n",
      "Epoch 29/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0446 - accuracy: 0.9902 - val_loss: 3.2473 - val_accuracy: 0.6651\n",
      "Epoch 30/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9719 - val_loss: 2.0605 - val_accuracy: 0.7037\n",
      "Epoch 31/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0376 - accuracy: 0.9877 - val_loss: 2.8332 - val_accuracy: 0.6806\n",
      "Epoch 32/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0402 - accuracy: 0.9877 - val_loss: 2.1948 - val_accuracy: 0.7037\n",
      "Epoch 33/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.9875 - val_loss: 2.5926 - val_accuracy: 0.6991\n",
      "Epoch 34/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.9902 - val_loss: 1.5530 - val_accuracy: 0.6836\n",
      "Epoch 35/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9899 - val_loss: 2.1075 - val_accuracy: 0.6975\n",
      "Epoch 36/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9959 - val_loss: 2.3466 - val_accuracy: 0.6852\n",
      "Epoch 37/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9847 - val_loss: 1.9310 - val_accuracy: 0.7022\n",
      "Epoch 38/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9888 - val_loss: 2.0774 - val_accuracy: 0.6651\n",
      "Epoch 39/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9916 - val_loss: 2.5316 - val_accuracy: 0.6944\n",
      "Epoch 40/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9834 - val_loss: 1.8158 - val_accuracy: 0.7006\n",
      "Epoch 41/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.9918 - val_loss: 2.2268 - val_accuracy: 0.6682\n",
      "Epoch 42/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9984 - val_loss: 3.0094 - val_accuracy: 0.6698\n",
      "Epoch 43/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 2.9713 - val_accuracy: 0.6975\n",
      "Epoch 44/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 0.9973 - val_loss: 2.5753 - val_accuracy: 0.6759\n",
      "Epoch 45/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.9937 - val_loss: 1.7501 - val_accuracy: 0.6836\n",
      "Epoch 46/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.9850 - val_loss: 2.1384 - val_accuracy: 0.6790\n",
      "Epoch 47/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9907 - val_loss: 3.2531 - val_accuracy: 0.6636\n",
      "Epoch 48/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0440 - accuracy: 0.9880 - val_loss: 2.0996 - val_accuracy: 0.7052\n",
      "Epoch 49/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9875 - val_loss: 1.6265 - val_accuracy: 0.6960\n",
      "Epoch 50/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0508 - accuracy: 0.9858 - val_loss: 1.7440 - val_accuracy: 0.6759\n",
      "Epoch 51/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.9905 - val_loss: 2.4813 - val_accuracy: 0.6960\n",
      "Epoch 52/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.9916 - val_loss: 2.6672 - val_accuracy: 0.7052\n",
      "Epoch 53/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9883 - val_loss: 2.9879 - val_accuracy: 0.7006\n",
      "Epoch 54/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 0.9842 - val_loss: 2.1562 - val_accuracy: 0.6867\n",
      "Epoch 55/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 2.5731 - val_accuracy: 0.6898\n",
      "Epoch 56/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 0.9984 - val_loss: 3.3508 - val_accuracy: 0.6991\n",
      "Epoch 57/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9929 - val_loss: 3.4468 - val_accuracy: 0.6481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.9834 - val_loss: 1.9742 - val_accuracy: 0.6975\n",
      "Epoch 59/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9913 - val_loss: 2.2389 - val_accuracy: 0.6867\n",
      "Epoch 60/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9877 - val_loss: 2.2287 - val_accuracy: 0.6898\n",
      "Epoch 61/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0274 - accuracy: 0.9916 - val_loss: 1.9187 - val_accuracy: 0.6836\n",
      "Epoch 62/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.9940 - val_loss: 3.0234 - val_accuracy: 0.6991\n",
      "Epoch 63/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9956 - val_loss: 2.1349 - val_accuracy: 0.6867\n",
      "Epoch 64/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9894 - val_loss: 2.3043 - val_accuracy: 0.7052\n",
      "Epoch 65/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 2.2951 - val_accuracy: 0.6867\n",
      "Epoch 66/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.9954 - val_loss: 1.9802 - val_accuracy: 0.6667\n",
      "Epoch 67/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.9937 - val_loss: 1.5569 - val_accuracy: 0.6975\n",
      "Epoch 68/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 2.1506 - val_accuracy: 0.6975\n",
      "Epoch 69/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9937 - val_loss: 2.0371 - val_accuracy: 0.7052\n",
      "Epoch 70/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 0.9973 - val_loss: 2.8323 - val_accuracy: 0.6975\n",
      "Epoch 71/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 2.9549 - val_accuracy: 0.6867\n",
      "Epoch 72/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9975 - val_loss: 5.2062 - val_accuracy: 0.6991\n",
      "Epoch 73/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0856 - accuracy: 0.9749 - val_loss: 1.8354 - val_accuracy: 0.6651\n",
      "Epoch 74/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.9867 - val_loss: 2.1965 - val_accuracy: 0.6960\n",
      "Epoch 75/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0457 - accuracy: 0.9867 - val_loss: 2.0116 - val_accuracy: 0.6883\n",
      "Epoch 76/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.9913 - val_loss: 2.1508 - val_accuracy: 0.6775\n",
      "Epoch 77/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.9926 - val_loss: 1.9202 - val_accuracy: 0.6821\n",
      "Epoch 78/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9981 - val_loss: 3.2948 - val_accuracy: 0.6991\n",
      "Epoch 79/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9989 - val_loss: 5.1784 - val_accuracy: 0.6929\n",
      "Epoch 80/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 5.5593 - val_accuracy: 0.6975\n",
      "Epoch 81/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9894 - val_loss: 2.1556 - val_accuracy: 0.6944\n",
      "Epoch 82/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0429 - accuracy: 0.9918 - val_loss: 3.9986 - val_accuracy: 0.6620\n",
      "Epoch 83/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9861 - val_loss: 3.5706 - val_accuracy: 0.6790\n",
      "Epoch 84/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9872 - val_loss: 2.6590 - val_accuracy: 0.6898\n",
      "Epoch 85/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.9937 - val_loss: 3.0915 - val_accuracy: 0.6898\n",
      "Epoch 86/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0126 - accuracy: 0.9975 - val_loss: 1.9696 - val_accuracy: 0.7022\n",
      "Epoch 87/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9978 - val_loss: 4.8086 - val_accuracy: 0.7083\n",
      "Epoch 88/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.9926 - val_loss: 3.6893 - val_accuracy: 0.6929\n",
      "Epoch 89/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.9894 - val_loss: 2.7302 - val_accuracy: 0.6991\n",
      "Epoch 90/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.9937 - val_loss: 3.3515 - val_accuracy: 0.6790\n",
      "Epoch 91/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9894 - val_loss: 4.9418 - val_accuracy: 0.6975\n",
      "Epoch 92/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9856 - val_loss: 2.8558 - val_accuracy: 0.6929\n",
      "Epoch 93/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.9932 - val_loss: 1.1144 - val_accuracy: 0.7176\n",
      "Epoch 94/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0496 - accuracy: 0.9875 - val_loss: 2.1048 - val_accuracy: 0.6821\n",
      "Epoch 95/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.9948 - val_loss: 2.3684 - val_accuracy: 0.6960\n",
      "Epoch 96/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 4.9166 - val_accuracy: 0.7083\n",
      "Epoch 97/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 8.8050e-05 - accuracy: 1.0000 - val_loss: 5.5290 - val_accuracy: 0.7006\n",
      "Epoch 98/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 2.7663e-05 - accuracy: 1.0000 - val_loss: 5.9609 - val_accuracy: 0.7022\n",
      "Epoch 99/100\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 1.6212e-05 - accuracy: 1.0000 - val_loss: 6.3752 - val_accuracy: 0.7068\n",
      "Epoch 100/100\n",
      "115/115 [==============================] - 0s 4ms/step - loss: 1.0985e-05 - accuracy: 1.0000 - val_loss: 6.6885 - val_accuracy: 0.7068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b4b25d9550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize\n",
    "ann = tf.keras.models.Sequential()\n",
    "# add input layer and hidden layer\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dense(150, activation = 'relu'))\n",
    "# add output layer\n",
    "ann.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "ann.compile(\n",
    "    optimizer= 'adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Train the model\n",
    "ann.fit(X_train, \n",
    "        y_train, \n",
    "        batch_size = 32,\n",
    "        epochs=100, \n",
    "        verbose=1, \n",
    "        # Pass in a validation data set to test each epoch.\n",
    "        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did not help either. Perhaps we are using too many layers and we don't need that many since our dataset size is relatively small. We should also add dropout layers to prevent the model overfitting quickly. Note that many dropout rates starting from 0.2 up to 0.8 were tested and 0.7 seemed to be the sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "115/115 [==============================] - 0s 3ms/step - loss: 1.0357 - accuracy: 0.5116 - val_loss: 0.6868 - val_accuracy: 0.5093\n",
      "Epoch 2/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.8279 - accuracy: 0.5176 - val_loss: 0.6872 - val_accuracy: 0.5617\n",
      "Epoch 3/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7662 - accuracy: 0.5072 - val_loss: 0.6895 - val_accuracy: 0.5494\n",
      "Epoch 4/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7429 - accuracy: 0.5064 - val_loss: 0.6908 - val_accuracy: 0.5231\n",
      "Epoch 5/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7141 - accuracy: 0.5124 - val_loss: 0.6907 - val_accuracy: 0.5694\n",
      "Epoch 6/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7126 - accuracy: 0.5026 - val_loss: 0.6912 - val_accuracy: 0.6142\n",
      "Epoch 7/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.5217 - val_loss: 0.6904 - val_accuracy: 0.6173\n",
      "Epoch 8/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7014 - accuracy: 0.5326 - val_loss: 0.6902 - val_accuracy: 0.6003\n",
      "Epoch 9/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.7025 - accuracy: 0.5266 - val_loss: 0.6904 - val_accuracy: 0.6111\n",
      "Epoch 10/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6975 - accuracy: 0.5116 - val_loss: 0.6901 - val_accuracy: 0.5941\n",
      "Epoch 11/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5402 - val_loss: 0.6896 - val_accuracy: 0.5910\n",
      "Epoch 12/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5470 - val_loss: 0.6886 - val_accuracy: 0.5818\n",
      "Epoch 13/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6869 - accuracy: 0.5467 - val_loss: 0.6856 - val_accuracy: 0.6404\n",
      "Epoch 14/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6880 - accuracy: 0.5557 - val_loss: 0.6838 - val_accuracy: 0.6466\n",
      "Epoch 15/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.5669 - val_loss: 0.6773 - val_accuracy: 0.6312\n",
      "Epoch 16/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6804 - accuracy: 0.5710 - val_loss: 0.6713 - val_accuracy: 0.6420\n",
      "Epoch 17/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6742 - accuracy: 0.5759 - val_loss: 0.6636 - val_accuracy: 0.6543\n",
      "Epoch 18/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.5873 - val_loss: 0.6601 - val_accuracy: 0.6651\n",
      "Epoch 19/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6670 - accuracy: 0.5998 - val_loss: 0.6565 - val_accuracy: 0.6651\n",
      "Epoch 20/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.6015 - val_loss: 0.6451 - val_accuracy: 0.6728\n",
      "Epoch 21/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6660 - accuracy: 0.6037 - val_loss: 0.6486 - val_accuracy: 0.6667\n",
      "Epoch 22/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.6148 - val_loss: 0.6459 - val_accuracy: 0.6713\n",
      "Epoch 23/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6559 - accuracy: 0.6271 - val_loss: 0.6457 - val_accuracy: 0.6790\n",
      "Epoch 24/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6532 - accuracy: 0.6244 - val_loss: 0.6413 - val_accuracy: 0.6698\n",
      "Epoch 25/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6521 - accuracy: 0.6293 - val_loss: 0.6371 - val_accuracy: 0.6728\n",
      "Epoch 26/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.6301 - val_loss: 0.6266 - val_accuracy: 0.6790\n",
      "Epoch 27/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6452 - accuracy: 0.6372 - val_loss: 0.6380 - val_accuracy: 0.6744\n",
      "Epoch 28/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6355 - accuracy: 0.6472 - val_loss: 0.6297 - val_accuracy: 0.6728\n",
      "Epoch 29/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6363 - accuracy: 0.6535 - val_loss: 0.6293 - val_accuracy: 0.6713\n",
      "Epoch 30/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6422 - accuracy: 0.6442 - val_loss: 0.6300 - val_accuracy: 0.6744\n",
      "Epoch 31/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6480 - accuracy: 0.6363 - val_loss: 0.6341 - val_accuracy: 0.6728\n",
      "Epoch 32/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6336 - accuracy: 0.6551 - val_loss: 0.6300 - val_accuracy: 0.6806\n",
      "Epoch 33/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6379 - accuracy: 0.6576 - val_loss: 0.6342 - val_accuracy: 0.6682\n",
      "Epoch 34/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6387 - accuracy: 0.6516 - val_loss: 0.6271 - val_accuracy: 0.6790\n",
      "Epoch 35/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6329 - accuracy: 0.6516 - val_loss: 0.6283 - val_accuracy: 0.6775\n",
      "Epoch 36/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6305 - accuracy: 0.6603 - val_loss: 0.6297 - val_accuracy: 0.6883\n",
      "Epoch 37/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6356 - accuracy: 0.6478 - val_loss: 0.6285 - val_accuracy: 0.6836\n",
      "Epoch 38/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6296 - accuracy: 0.6589 - val_loss: 0.6231 - val_accuracy: 0.6821\n",
      "Epoch 39/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6261 - accuracy: 0.6581 - val_loss: 0.6230 - val_accuracy: 0.6806\n",
      "Epoch 40/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6278 - accuracy: 0.6606 - val_loss: 0.6241 - val_accuracy: 0.6821\n",
      "Epoch 41/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6322 - accuracy: 0.6505 - val_loss: 0.6197 - val_accuracy: 0.6775\n",
      "Epoch 42/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6315 - accuracy: 0.6649 - val_loss: 0.6230 - val_accuracy: 0.6836\n",
      "Epoch 43/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6211 - accuracy: 0.6663 - val_loss: 0.6222 - val_accuracy: 0.6806\n",
      "Epoch 44/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.6693 - val_loss: 0.6153 - val_accuracy: 0.6883\n",
      "Epoch 45/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.6644 - val_loss: 0.6186 - val_accuracy: 0.6867\n",
      "Epoch 46/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6165 - accuracy: 0.6682 - val_loss: 0.6171 - val_accuracy: 0.6867\n",
      "Epoch 47/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6341 - accuracy: 0.6614 - val_loss: 0.6357 - val_accuracy: 0.6836\n",
      "Epoch 48/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6145 - accuracy: 0.6690 - val_loss: 0.6175 - val_accuracy: 0.6821\n",
      "Epoch 49/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.6696 - val_loss: 0.6219 - val_accuracy: 0.6852\n",
      "Epoch 50/50\n",
      "115/115 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.6846 - val_loss: 0.6159 - val_accuracy: 0.6852\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "ann = tf.keras.models.Sequential()\n",
    "# add input layer and hidden layer\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dropout(0.7)) # Set 20% of the nodes to 0\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dropout(0.7))\n",
    "ann.add(tf.keras.layers.Dense(100, activation = 'relu'))\n",
    "ann.add(tf.keras.layers.Dropout(0.8))\n",
    "\n",
    "# add output layer\n",
    "ann.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "ann.compile(\n",
    "    optimizer= 'adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Train the model\n",
    "history = ann.fit(X_train, \n",
    "        y_train, \n",
    "        batch_size = 32,\n",
    "        epochs=50, \n",
    "        verbose=1, \n",
    "        # Pass in a validation data set to test each epoch.\n",
    "        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_334 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Train Accuracy: 0.6846\n",
      "Test Accuracy: 0.6121\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network\n",
    "train_accuracy = history.history[\"accuracy\"][-1]\n",
    "result = ann.evaluate(X_test,y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {result[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, even though this final model did not overfit, we got a 61% accuracy at the end which could just be bad luck due to the small test size. However, the SVC model gave me more consistent results at higher accuracies so I will chose that as my final model.\n",
    "\n",
    "Lets now plot the model loss and accuracy scores over the training period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.savefig('ann.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.savefig('annac.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is no longer overfitting and the train and validation accuracies are close to each other which is a good sign that we have picked out our hyperparameters wisely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
